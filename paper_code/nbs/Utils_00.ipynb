{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e70af5-647b-4cdd-8a9a-4d34eba84dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import SimpleITK as sitk\n",
    "import skimage\n",
    "import torch\n",
    "import torchmetrics\n",
    "from monai.transforms import Transform\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adfd2298-5453-42ac-a220-eb5ae744e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCLAHE(Transform):\n",
    "    \"\"\"Implements Contrast-Limited Adaptive Histogram Equalization (CLAHE) as a custom transform, as described by Qiu et al.\n",
    "\n",
    "    Attributes:\n",
    "        p1 (float): Weighting factor, determines degree of of contour enhacement. Default is 0.6.\n",
    "        p2 (None or int): Kernel size for adaptive histogram. Default is None.\n",
    "        p3 (float): Clip limit for histogram equalization. Default is 0.01.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p1=0.6, p2=None, p3=0.01):\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.p3 = p3\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"Apply the CLAHE algorithm to input data.\n",
    "\n",
    "        Args:\n",
    "            data (Union[dict, np.ndarray]): Input data. Could be a dictionary containing the image or the image array itself.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed data.\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            im = data[\"image\"]\n",
    "\n",
    "        else:\n",
    "            im = data\n",
    "        im = im.numpy()\n",
    "        im = skimage.exposure.rescale_intensity(im, in_range=\"image\", out_range=(0, 1))\n",
    "        im_noi = skimage.filters.median(im)\n",
    "        im_fil = im_noi - self.p1 * skimage.filters.gaussian(im_noi, sigma=1)\n",
    "        im_fil = skimage.exposure.rescale_intensity(im_fil, in_range=\"image\", out_range=(0, 1))\n",
    "        im_ce = skimage.exposure.equalize_adapthist(im_fil, kernel_size=self.p2, clip_limit=self.p3)\n",
    "        if isinstance(data, dict):\n",
    "            data[\"image\"] = torch.Tensor(im_ce)\n",
    "        else:\n",
    "            data = torch.Tensor(im_ce)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f551d35f-ade6-40c7-81a4-e21393127180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_metrics(metrics_path):\n",
    "    \"\"\"Read a CSV logging file and display it in a plot to visualize training progress.\n",
    "\n",
    "    Args:\n",
    "        metrics_path (str): Path to the CSV file containing training metrics.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plot and prints maximum values for each metric.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(metrics_path)\n",
    "    del df[\"step\"]\n",
    "    df.set_index(\"epoch\", inplace=True)\n",
    "    sn.relplot(data=df, kind=\"line\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "    max_acc = df.max(axis=0)\n",
    "    print(f\"max values: \\n{max_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feccbf6-df87-4345-a053-a83c2d5caa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_GradCAMs(image_paths, model, transform):\n",
    "    \"\"\"Create GradCAMs for a given list of image paths and a model.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list of str): List of file paths to the images for which GradCAMs are to be created.\n",
    "        model (torch.nn.Module): PyTorch model for which GradCAMs are generated.\n",
    "        transform (torchvision.transforms.Compose): Transform pipeline for preprocessing images.\n",
    "\n",
    "    Returns:\n",
    "        list of tuple: List containing tuples of original images and their corresponding GradCAMs.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    target_layers = [model.model.layer4[-1]]\n",
    "\n",
    "    visualization_list = []\n",
    "\n",
    "    pbar = tqdm(total=len(image_paths))\n",
    "    for i in image_paths:\n",
    "        image_only = transform(i)\n",
    "        image_only = image_only.unsqueeze(0)\n",
    "        arr = image_only.numpy().squeeze()\n",
    "        arr = arr[..., None]\n",
    "        cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
    "        targets = None\n",
    "        grayscale_cam = cam(\n",
    "            input_tensor=image_only,\n",
    "            targets=targets,\n",
    "            aug_smooth=False,\n",
    "            eigen_smooth=True,\n",
    "        )\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        visualization_list.append((arr, grayscale_cam))\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return visualization_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1ebd7b-3661-4ce3-a4ab-589fb619682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_in_grid(image_list, num_col=4, save_path=None):\n",
    "    \"\"\"Visualize input GradCAMs as an image grid for inspection.\n",
    "\n",
    "    Args:\n",
    "        image_list (list of np.ndarray): List containing GradCAM images.\n",
    "        num_col (int, optional): Number of columns in the grid. Defaults to 4.\n",
    "        save_path (str, optional): File path to save the image grid. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the image grid and optionally saves it to a file.\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Displaying {len(image_list)} images in the grid:\")\n",
    "    fig = plt.figure(figsize=(20, 40))\n",
    "    jet = plt.colormaps.get_cmap(\"inferno\")\n",
    "    newcolors = jet(np.linspace(0, 1, 256))\n",
    "    newcolors[0, :3] = 0\n",
    "    new_jet = mcolors.ListedColormap(newcolors)\n",
    "    grid = ImageGrid(\n",
    "        fig,\n",
    "        111,\n",
    "        nrows_ncols=(\n",
    "            round(len(image_list) / num_col) + 1,\n",
    "            num_col,\n",
    "        ),\n",
    "        axes_pad=0.1,\n",
    "    )\n",
    "    pbar = tqdm(total=len(image_list))\n",
    "    for ax, im in zip(grid, image_list):\n",
    "        ax.grid(False)\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(im[0], cmap=\"gray\")\n",
    "        ax.imshow(im[1], alpha=0.5, cmap=new_jet)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686ba300-fe68-4df4-85e3-c6be8e36d1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classification_report(preds, targets, threshhold=0.5):\n",
    "    \"\"\"Generate and print a classification report.\n",
    "\n",
    "    Args:\n",
    "        preds (torch.Tensor): Tensor containing the predicted probabilities.\n",
    "        targets (torch.Tensor): Tensor containing the true target values.\n",
    "        threshhold (float, optional): Classification threshold for predictions. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the classification report to the console.\n",
    "\n",
    "    \"\"\"\n",
    "    threshold = threshhold\n",
    "\n",
    "    targets_array = targets[:,1].numpy().astype(int)\n",
    "    predictions_array = np.where(preds[:,1].numpy()>threshold, 1, 0)\n",
    "\n",
    "    print(metrics.classification_report(y_true=targets_array, y_pred=predictions_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d2f9c-fb48-4229-9b76-1c34825509d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probs_hist(preds, targets, threshhold, save_path=None, legend_pos=\"best\", y_ticks=10):\n",
    "    \"\"\"Visualize the model output probabilities for each class using a histogram.\n",
    "\n",
    "    Args:\n",
    "        preds (torch.Tensor): Tensor containing the predicted probabilities.\n",
    "        targets (torch.Tensor): Tensor containing the true target values.\n",
    "        threshhold (float): Cut-off value for classification.\n",
    "        save_path (str, optional): File path to save the plot. Defaults to None.\n",
    "        legend_pos (str or tuple, optional): The position of the legend. Defaults to \"best\".\n",
    "        y_ticks (int): Spacing of the ticks on the y-axis\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the histogram plot and optionally saves it to a file.\n",
    "\n",
    "    \"\"\"\n",
    "    cut_off = threshhold\n",
    "    data = pd.DataFrame({\"prob_preds\": preds[:, 1].numpy(), \"targs\": targets[:, 1].numpy()})\n",
    "\n",
    "    plt.style.use(\"default\")\n",
    "    hist_plot = sn.histplot(\n",
    "        data=data,\n",
    "        x=\"prob_preds\",\n",
    "        hue=\"targs\",\n",
    "        bins=20,\n",
    "        multiple=\"dodge\",\n",
    "        palette={0: \"#005AB5\", 1: \"#DC3220\"},\n",
    "\n",
    "    )  # , edgecolor=None)\n",
    "    ymax = np.max([p.get_height() for p in hist_plot.patches])\n",
    "    plt.vlines(x=cut_off, color=\"gray\", linestyle=\"--\", ymin=0, ymax=ymax)\n",
    "\n",
    "    plt.xlabel(\"Probabillities\", fontsize=16)\n",
    "    plt.ylabel('Count', fontsize=16)\n",
    "\n",
    "    plt.yticks(np.arange(0, plt.ylim()[1], y_ticks))\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=12)\n",
    "\n",
    "    plt.legend(labels=[\"cut-off value\", \"1\", \"0\"], frameon=False,fontsize=12, loc=legend_pos)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149abb03-a8eb-41d0-815c-fb2aba12b5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sensitivity_vs_FPR_val(preds, targets, threshhold, save_path=None, loc=None):\n",
    "    \"\"\"Create a plot of sensitivity versus false positive rate (FPR) based on varying threshold values.\n",
    "\n",
    "    Args:\n",
    "        preds (torch.Tensor): Tensor containing the predicted values.\n",
    "        targets (torch.Tensor): Tensor containing the true target values.\n",
    "        threshhold (float): Cut-off value for classification.\n",
    "        save_path (str, optional): File path to save the plot. Defaults to None.\n",
    "        loc (str, optional): Position of the Legend in the graph. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plot of sensitivity versus FPR and optionally saves it to a file.\n",
    "\n",
    "    \"\"\"\n",
    "    cut_off = threshhold\n",
    "    thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "    recall_list = []\n",
    "    FPR_list = []\n",
    "    for i in thresholds:\n",
    "        recall_list.append(\n",
    "            torchmetrics.functional.classification.binary_recall(preds[:,1], targets[:,1].long(), threshold=i)\n",
    "        )\n",
    "        FPR_list.append(\n",
    "            1\n",
    "            - (\n",
    "                torchmetrics.functional.classification.binary_specificity(\n",
    "                    preds[:,1], targets[:,1].long(), threshold=i\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    y_max = np.interp(cut_off, thresholds, recall_list)\n",
    "    y_min = np.interp(cut_off, thresholds, FPR_list)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.style.use(\"default\")\n",
    "\n",
    "    ax.plot(thresholds, recall_list, linewidth=2, c=\"#005AB5\")\n",
    "    ax.plot(thresholds, FPR_list, linewidth=2, c=\"#DC3220\")\n",
    "    ax.vlines(x=cut_off, color=\"gray\", linestyle=\"--\", ymax=y_max, ymin=y_min)\n",
    "\n",
    "    ax.set_ylabel(\"Sensitivity / False positive rate\",fontsize=16)\n",
    "    ax.set_xlabel(\"cut-off value\",fontsize=16)\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=12)\n",
    "\n",
    "    ax.legend([\"Sensitivity\", \"False positive rate\", \"cut-off value\"], frameon=False, loc=loc, fontsize=12)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fea1b5-b2ee-4be3-a8ed-6990e255f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(preds, targets, threshhold=0.5, save_path=None):\n",
    "    \"\"\"Create and visualize a confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        preds (torch.Tensor): Tensor containing the predicted values.\n",
    "        targets (torch.Tensor): Tensor containing the true target values.\n",
    "        threshhold (float, optional): Classification threshold for predictions. Defaults to 0.5.\n",
    "        save_path (str, optional): File path to save the plot. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the confusion matrix and optionally saves it to a file.\n",
    "\n",
    "    \"\"\"\n",
    "    threshold = threshhold\n",
    "\n",
    "    targets_array = targets[:,1].numpy().astype(int)\n",
    "    predictions_array = np.where(preds[:,1].numpy()>threshold, 1, 0)\n",
    "\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true=targets_array, y_pred=predictions_array)\n",
    "    class_labels = [\"nr-AxSpA\", \"r-AxSpA\"]\n",
    "\n",
    "    plt.style.use(\"default\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sn.heatmap(\n",
    "        confusion_matrix,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        fmt=\"d\",\n",
    "        xticklabels=class_labels,\n",
    "        yticklabels=class_labels,\n",
    "        cbar=True,\n",
    "        annot_kws={\"size\": 16}\n",
    "    )\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=16)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=16)\n",
    "\n",
    "    plt.title(\"Confusion matrix\",fontsize=20)\n",
    "    plt.xlabel(\"Prediction\",fontsize=18)\n",
    "    plt.ylabel(\"Ground truth\",fontsize=18)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf424f01-d535-4054-a1df-13a1e636988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_plot(preds, targets, save_path=None):\n",
    "    \"\"\"Plot the Receiver Operating Characteristic (ROC) curve and calculate the area under the curve (AUC).\n",
    "\n",
    "    Args:\n",
    "        preds (torch.Tensor): Tensor containing the predicted values.\n",
    "        targets (torch.Tensor): Tensor containing the true target values.\n",
    "        save_path (str, optional): File path to save the plot. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plot and optionally saves it to a file.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    targets_array = targets[:,1].numpy().astype(int)\n",
    "    predictions_array = preds[:,1].numpy()\n",
    "\n",
    "    roc = metrics.roc_curve(targets_array, predictions_array)\n",
    "\n",
    "    auc = round(metrics.roc_auc_score(targets_array, predictions_array), 3)\n",
    "\n",
    "    p = [0, 1]\n",
    "    i = [0, 1]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.style.use(\"default\")\n",
    "    ax.plot(roc[0], roc[1], linestyle=\"-\", c=\"#DC3220\", linewidth=2)\n",
    "    ax.plot(p, i, linestyle=\"dotted\", c=\"0.5\")\n",
    "\n",
    "    ax.set_title(\"Receiver operating characteristics curve\", loc=\"center\",fontsize=20)\n",
    "    ax.set_xlabel(\"False positive rate\",fontsize=18)\n",
    "    ax.set_ylabel(\"True positive rate\",fontsize=18)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=12)\n",
    "    plt.text(\n",
    "        0.955,\n",
    "        0.05,\n",
    "        f\"AUC = {str(auc)}\",\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"bottom\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=16\n",
    "    )\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621ab883-1a5d-41d0-a3e7-c02a8f307a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_w_CI(preds, metric, metric_name=None, threshold=0.5, return_values=False):\n",
    "    \"\"\"Calculate confidence intervals with bootstrapping for an input metric.\n",
    "\n",
    "    Args:\n",
    "        preds (dict): A dictionary of tensors containing the predictions and targets.\n",
    "            Expected keys are \"preds\" and \"targets\".\n",
    "        metric (torchmetrics.Metric): A torchmetrics metric function for which the confidence interval should be calculated.\n",
    "        metric_name (str, optional): The name to be displayed next to the metric. Defaults to None.\n",
    "        return_values (bool, optional): Whether to return the calculated values. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: If return_values is True, returns a dictionary containing the metric, lower bound, and upper bound.\n",
    "        Otherwise, prints these values and returns None.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    bootstrapped_metric = torchmetrics.wrappers.BootStrapper(\n",
    "        metric,\n",
    "        num_bootstraps=1000,\n",
    "        mean=False,\n",
    "        std=False,\n",
    "        raw=True,\n",
    "        sampling_strategy=\"multinomial\",\n",
    "    )\n",
    "    if metric_name== \"AUROC\":\n",
    "        predictions = preds[\"preds\"][:,1]\n",
    "    else:\n",
    "        predictions = torch.where(preds[\"preds\"][:,1]>threshold,1,0)\n",
    "    targets = preds[\"targets\"][:,1]\n",
    "    bootstrapped_metric.update(predictions, targets)\n",
    "    metric_boot_result = bootstrapped_metric.compute()\n",
    "    metric_result = round(float(metric(predictions, targets)), 3)\n",
    "\n",
    "    lower_bound = np.percentile(metric_boot_result[\"raw\"], 2.5)\n",
    "    upper_bound = np.percentile(metric_boot_result[\"raw\"], 97.5)\n",
    "\n",
    "    if metric_name is None:\n",
    "        metric_name = str(metric)\n",
    "\n",
    "    if return_values is True:\n",
    "        return {\n",
    "            metric_name: metric_result,\n",
    "            \"lower_bound\": round(lower_bound, 3),\n",
    "            \"upper_bound\": round(upper_bound, 3),\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"{metric_name}: {metric_result} (95% CI: {round(lower_bound,3)}, {round(upper_bound, 3)})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88344fc4-fe11-49fc-88b0-c864d2a61c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_comparison(\n",
    "    first_set,\n",
    "    second_set,\n",
    "    labels=None,\n",
    "    save_path=None,\n",
    "    return_res=False,\n",
    "):\n",
    "    \"\"\"Compares the sizes of images in two sets, plots the distribution, and optionally returns resolutions.\n",
    "\n",
    "    Args:\n",
    "        first_set (list): A list of file paths for the images in the first set.\n",
    "        second_set (list): A list of file paths for the images in the second set.\n",
    "        labels (list, optional): A list of two strings representing labels for the first and second sets in the plot.\n",
    "            Defaults to ['first set', 'second set'].\n",
    "        save_path (str, optional): The file path where the plot should be saved. If None, the plot is not saved.\n",
    "            Defaults to None.\n",
    "        return_res (bool, optional): A flag to indicate whether the resolutions of the images in both sets should be returned.\n",
    "            Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: If return_res is True, returns a tuple of two lists containing the resolutions of the images in the\n",
    "                       first and second sets.\n",
    "                       Otherwise, displays a plot and optionally saves it, but does not return any values.\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = [\"first set\", \"second set\"]\n",
    "\n",
    "    resolutions_list_first_set = []\n",
    "    for i in first_set[:30]:\n",
    "        reader = sitk.ImageFileReader()\n",
    "        reader.SetFileName(i)\n",
    "        reader.ReadImageInformation()\n",
    "        size = reader.GetSize()\n",
    "        res = np.sqrt(size[0] * size[1])\n",
    "        resolutions_list_first_set.append(res)\n",
    "\n",
    "    resolutions_list_second_set = []\n",
    "    for i in second_set[:30]:\n",
    "        reader = sitk.ImageFileReader()\n",
    "        reader.SetFileName(i)\n",
    "        reader.ReadImageInformation()\n",
    "        size = reader.GetSize()\n",
    "        res = np.sqrt(size[0] * size[1])\n",
    "        resolutions_list_second_set.append(res)\n",
    "\n",
    "    size_data = pd.DataFrame(\n",
    "        {\n",
    "            \"values\": resolutions_list_first_set + resolutions_list_second_set,\n",
    "            \"type\": [\"top losses\"] * len(resolutions_list_first_set)\n",
    "            + [\"min losses\"] * len(resolutions_list_second_set),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    plt.style.use(\"default\")\n",
    "    sn.histplot(\n",
    "        data=size_data,\n",
    "        x=\"values\",\n",
    "        hue=\"type\",\n",
    "        bins=20,\n",
    "        multiple=\"dodge\",\n",
    "        palette={\"top losses\": \"#DC3220\", \"min losses\": \"#005AB5\"},\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.legend(labels, frameon=False)\n",
    "\n",
    "    if return_res:\n",
    "        return resolutions_list_first_set, resolutions_list_second_set\n",
    "\n",
    "    else:\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
